Original Baseline results:


Number of epochs to reach a validation accuracy of 0.4: Epoch 3
Validation accuracy after 100 epochs (batch_size = 100): 0.645

Add your names (all your names), a brief description of the
modification you made (what's on the assignment is fine) and the summary results:

A. Names: Nirman Dave, Mei Zhou, Alina Dao
Description: use momentum optimization with a momentum of 0.9, learning rate 0.001
Summary of Results:
Number of epochs to reach a validation accuracy of 0.4: Epoch 9
Validation accuracy after 100 epochs: 0.64432889
Note: We ran the program to completion at 1000 Epoches. 
We found out that the validation accuracy did not ever reach 0.66. At around 150 epoches, it 
started to fluctuate between 0.65 and 0.648/0.649.  

B. Names: Sarah Zuckerman, Thyne Boonmark, Cole Stephens
Summary of Results:
No random number seed worked with momentum of 0.9 and learning rate 0.01
Eventually changed momentum to 0.5, Learning Rate of 0.01
Number of epochs to reach a validation accuracy of 0.4: Epoch 4
Validation accuracy after 100 Epochs: 0.66
Stopped at Epoch 237 with Validation Accuracy of 0.67

D. Names: Ian Nanez, Hutomo Limanto, Obi Daniel Ezeogu
Description: use momentum optimization with a momentum of 0.5, learning rate 0.001
Summary Results:
Number of epochs to reach a validation accuracy of 0.4: Epoch 20
Validation accuracy after 100 epochs (batch_size = 100): 0.59751952

E. Names: Evelyn Ting, Zach Bessette, Saharsha Karki
Description: use momentum optimization with a momentum of 0.5, learning rate 0.01
Summary of Results:
Number of epochs to reach a validation accuracy of 0.4: Epoch 3
Validation accuracy after 100 epochs (batch_size = 100): 0.664233
Note:  Zach and Saharsha’s programs got stuck with the original seed.  

Evelyn’s never did.  Also, Evelyn’s validation accuracy was higher than Zach and Saharsha’s for each 
epoch for each run (even though we had the same code).

F. Names: Sarah Teichman, Lehua Matsumoto, Ella Yarmo-Gray
Description: Keep percentage to 0.9, fully connected hidden layer of 256
Number of epochs to reach a validation accuracy of 0.4: Epoch 3
Validation accuracy after 100 epochs: 0.63272655
Validation accuracy reached 0.59 at epoch 7 and then began to increase with slight fluctuation 
(about 0.1 within 10 epochs)

G. Names: Mahesh Dhulipala, Ian McClaugherty, Bishesh Yadav
Description: Keep percentage to 0.5, fully connected hidden layer of 256
Summary Results:
Number of epochs to reach a validation accuracy of 0.4: 2
Validation accuracy after 100 epochs: 0.64592916
Note: Validation accuracy after 708 epochs: 0.62652528

H. Names: Robbie Zielinski, Kevin Goldberg, Andy Tang
Description: Keep percentage to 0.75, fully connected hidden layer of 1536
Number of epochs to reach a validation accuracy of 0.4: 3
Validation accuracy after 100 epochs: 0.56511301

I. Name: Sean Fitzgerald, Reynaldo Peña, Juan Guaracao
Description: Use momentum optimization with a momentum of 0.5, learning rate 0.1
Summary Results:
Number of epochs to reach a validation accuracy of 0.4: 4
Validation accuracy after 100 epochs: 0.66313261
Note: Validation accuracy after 265 epochs: 0.66433287

J. Names: Nicholas Lafky, Connor Haley, Phong Hoang
Description: change keep percentage to 0.9, fully connected hidden layer of 1536
Summary Results:
Number of epochs to reach a validation accuracy of 0.4: Never
Validation accuracy after 100 epochs (batch_size = 100):0.368874
The validation accuracy actually decreased as we ran it, going to 0.363273 after 240 epochs

K. Name: Mahima Ghale, Allison Lee, Alex Watson
Description: Increase first convolution layer to 64 with learning rate 0.01
Summary Results:
Number of epochs to reach validation of 0.4: 12
Validation accuracy after 100 epochs: 0.645
Validation accuracy after 305 epochs: 0.655

L: Names: Jeff Ewing, Sam Procter, Jake Meyer
Description: changed the number of features in the first conv layer to 64' learning rate to 0.05
Summary Results:
Number of epochs to reach a validation accuracy of 0.4: Epoch 2
Validation accuracy after 100 epochs (batch_size = 100): 0.65613121
Note: the first time running, our program got stuck. 


M. Names: Daniel Mariselli, Alex Chou, Carina Corbin
Description: Change the number of features in the second conv layer to 128, learning rate of 0.01
Summary Results:
Number of epochs to reach a validation accuracy of 0.4: Epoch 7
Validation accuracy after 100 epochs (batch_size = 100): 0.63772756

N. Names: Pierre-Alexander Low, Gabriel Young
Description: Change the number of features in the second conv later to 128, learning rate of 0.05
Summary Results:
Number of epochs to reach a validation accuracy of 0.4: Epoch 4 (0.47249451)
Validation accuracy after 100 epochs (batch_size = 100): 0.62412483
Program was stuck on 4532 seed on multiple attempts, seed changed to 4533

O. Names: Andrew Kim and Emily Masten
Description: Keep percentage at 0.5; fully connected hidden layer of size 1536
(second convolution layer size = 32)
Summary results:
Number of epochs to reach a validation accuracy of 0.4: 8
Validation accuracy after 100 epochs (batch_size = 100): 0.62292457

P. Names: Zeina Amhaz, Patrick Liu
Description: Add a second fully connected layer but make the first 256 and second 512
Summary Results:
Number of epochs to reach a validation accuracy of 0.4: 5
Validation accuracy after 100 epochs (batch_size = 100): 0.64132828

Q. Names: Hunter Voegele, Laura Schmidlein, Andrew Ferrero

Description: Compare the original pooling to compare this to tf.nn.avg_pool

Notes:
tf.nn_avg_pool() worked much better than the original pooling. Not only did it get to a high 
validation and test accuracy much faster than the original pooling, it also avoided overfitting 
for many more epochs. Although the accuracy was lower at 100 epochs, this is because the accuracy fluctuated 
from .64 - .65 until the 115 epoch, then overfitting drove the accuracy down gradually (much less gradual than 
the original).

tf.nn_avg_pool() statistics:
Number of epochs to reach a validation accuracy of 0.4: Epoch 1
Validation accuracy after 100 epochs (batch_size = 100): 0.640728

Original baseline statistics:
Number of epochs to reach a validation accuracy of 0.4: Epoch 3
Validation accuracy after 100 epochs (batch_size = 100): 0.645

R. Names: Megan Root, Harrison Marick, Dorit Song
Description: Changed dropout keep_prob to 0.8 and made stride length of one max pool layer 1
Number of Epochs to reach validation accuracy of 0.4: 5
Validation accuracy after 100 epochs (batch_size=100): 0.592118

T. Names: Tiffany Kha, Azka Javaid, Jordan Browning 
Description: add another convu/relu/maxpool layer but keep input to fully connected layer at 
3x3 compare to tf.nn.avg_pool
Summary results:
Number of epochs to reach a validation accuracy of 0.4: Epoch 2 (previously with max pool Epoch 8)
Validation accuracy after 100 epochs (batch_size = 100): 0.70003998 [previously with maxpool .5663]

U. Names: Amal Buford, Mohammed Ibrahim, Oluwatobi Oni-Orisan
Description: Add another conv/relu/maxpool layer while
    maintaining the fully connected layer at 6x6, compare to tf.nn.avg_pool
Summary Results:
    Number of epochs to reach a validation accuracy of 0.4: 2 (previously 5 with maxpool)
    Validation accuracy after 100 epochs (batch_size = 100): 0.66673332 (previously 0.6365273 with maxpool)

V. Names: Evan Wolf, Brendan Routh, Gab Conclaves
Description: add a second fully connected layer of size 256
Summary results:
Number of epochs to reach a validation accuracy of 0.4: 25
Validation accuracy after 100 epochs (batch_size = 100): 0.658532

W. Names: Jackson Herrick, Drew Kelleher
Description: Add a second fully connected layer but make the first 256 and the second 128
Summary results:
Number of epochs to reach a validation accuracy of 0.4: 3
Validation accuracy after 100 epochs (batch size 100): 0.64172834

X. Names: Ehsan Mojahedi, Jack Muller, Olivia Pinney
Description: "Use a learning rate that increases if something is going well dropping and decreases if going badly."
Summary of Results:
Number of epochs to reach a validation accuracy of 0.4: Epoch 6
Validation accuracy after 100 epochs: 0.10482097
Note: Our validation and test accuracy dropped sharply at epoch 88. This issue could be avoided by implementing
more complex logic to change the learning rate. Improved logic would only increase or decrease the learning
rate if there is a significant change in loss (avg_cost) between epochs. 

